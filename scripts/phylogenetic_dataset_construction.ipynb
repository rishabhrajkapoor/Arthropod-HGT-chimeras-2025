{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe89e93f-6b5f-46cc-953f-824cd18349ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home11/rkapoor/.conda/envs/rishabh/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import subprocess\n",
    "import ast\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from ete3 import NCBITaxa\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0532889-66ea-4bd6-b858-b12d33b3cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load filtered round2 blast chimeras\n",
    "import pickle\n",
    "file_path = 'outputs/clustered_ankyrin_transposon_secondary_filtered_chimeras.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    chimeras=pickle.load(file)\n",
    "##append to intervals\n",
    "intervals=[]\n",
    "for c in chimeras:\n",
    "    for i in chimeras[c]:\n",
    "        intervals.append(c+\";\"+chimeras[c][i]+\"_\"+str(i).replace(\" \",\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a13537e-a288-47e8-bada-0a0427134bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load a dictionary of primary:secondary chimera mappings for PCR-validated secondary chimeras from a previous pipeline iteration\n",
    "## this is to prioritize selection of pcr'd secondary chimeras \n",
    "import pickle\n",
    "file_path = 'outputs/previous_iteration_secondary_chimeras.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    previous_iteration_secondary=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "760e801a-5408-453c-ac6f-a11befe2ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load dictionary of secondary chimeras\n",
    "file_path = 'outputs/secondary_chimera_adjacency_list.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    secondary_chimera_adjacency_list=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02e51dc-4110-4241-a0b8-18d34b504aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "blast_hits=[]\n",
    "for chimera in chimeras:\n",
    "    ints=[x for x in intervals if chimera in x]\n",
    "    for x in ints:\n",
    "        a2=SeqIO.to_dict(SeqIO.parse(f'outputs/hmmbuild/{chimera}/{x}/sub_seq.fasta', 'fasta'))\n",
    "        if len(a2.keys())<=1:\n",
    "            ##extract sequences that have only self blast-hits (bit-score>min(bit-score non-arthropod))\n",
    "            ##blast hits instead of hmmsearch hits are used for phylogenetic dataset contstruction\n",
    "            blast_hits.append(x)\n",
    "hmmer_hits=set(intervals)-set(blast_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "272a2df1-ab8c-415b-8ecd-13e9b4e4c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load a dataframe of genome taxids from genome accessions\n",
    "df1=pd.read_csv('Data/genbank_genomes_4_22_2025.tsv',sep='\\t')\n",
    "df2=pd.read_csv('Data/refseq_genomes_scaffold_plus_4_19_2025.tsv',sep='\\t')\n",
    "dftax=pd.concat([df1,df2]).set_index('Assembly Accession')\n",
    "dftax.loc['GCF_006496715.1',['Organism Name','Organism Taxonomic ID']]=['Aedes albopictus',7160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80488c74-b2da-4f24-a0f8-6ed294734681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for a given primary representative chimera, finds a single secondary chimera  per taxid\n",
    "selects hits with maximum bitscore to the max length hgt interval per taxid\n",
    "\"\"\"\n",
    "def pick_secondaries(c):\n",
    "    \n",
    "    ##pick HGT interval with the maximum length \n",
    "    interval_map={x:x[1]-x[0] for x in chimeras[c] if chimeras[c][x]=='HGT'}\n",
    "    max_len=max(interval_map.values())\n",
    "    max_len_interval=str([k for k, v in interval_map.items () if v==max_len][0]).replace(\" \",\"\")\n",
    "    \n",
    "    ##selection if using hmmer hits\n",
    "    if max_len_interval not in blast_hits:\n",
    "    \n",
    "        arth_path = Path(f\"outputs/hmmsearch_v_arthropod/{c};HGT_{max_len_interval}.tsv\")\n",
    "        \n",
    "        arth = pd.read_csv(arth_path, sep=\"\\t\")\n",
    "        \n",
    "        self_taxid=arth[arth.target_name==c].taxid.values[0]\n",
    "        arth=arth[arth.taxid!=self_taxid]\n",
    "        arth=arth[arth.target_name.isin(secondary_chimera_adjacency_list[c])]\n",
    "        secondaries=set()\n",
    "        ## add previous iteration secondary chimeras to selection\n",
    "        prev=[]\n",
    "        if c in previous_iteration_secondary:\n",
    "            prev=previous_iteration_secondary[c]\n",
    "        if arth.shape[0]>0:\n",
    "            \n",
    "            if len(prev)>0:\n",
    "                for p in prev:\n",
    "                    if p!=c:\n",
    "                        parth=arth[arth.target_name==p]\n",
    "                        ptaxid=parth.taxid.values[0]\n",
    "                        secondaries.add(parth.target_name.values[0])\n",
    "                        arth=arth[arth.taxid!=ptaxid]\n",
    "            ##add maximum bit score per taxid\n",
    "            secondaries=secondaries|set(arth.loc[arth.groupby('taxid')['domain_score'].idxmax()].target_name)\n",
    "    ##selection if using blast hits\n",
    "    else:\n",
    "        arth_path = Path(f\"outputs/round2_diamond_v_arthropod_output_split/{c};HGT_{max_len_interval}.tsv\")\n",
    "        arth = pd.read_csv(arth_path, sep=\"\\t\")\n",
    "        arth['taxid']=[int(dftax.loc[x.split(\";\")[0],'Organism Taxonomic ID']) for x in arth['sseqid']]\n",
    "        self_taxid=arth[arth.sseqid==c].taxid.values[0]\n",
    "        arth=arth[arth.taxid!=self_taxid]\n",
    "        arth=arth[arth.target_name.isin(secondary_chimera_adjacency_list[c])]\n",
    "        secondaries=set()\n",
    "        prev=[]\n",
    "        if c in previous_iteration_secondary:\n",
    "            prev=previous_iteration_secondary[c]\n",
    "        if arth.shape[0]>0:\n",
    "            if len(prev)>0:\n",
    "                for p in prev:\n",
    "                    if p!=c:\n",
    "                        parth=arth[arth.sseqid==p]\n",
    "                        ptaxid=parth.taxid.values[0]\n",
    "                        secondaries.add(parth.target_name.values[0])\n",
    "                        arth=arth[arth.taxid!=ptaxid]\n",
    "        secondaries=secondaries|set(arth.loc[arth.groupby('taxid')['bitscore'].idxmax()].target_name)\n",
    "        \n",
    "    return secondaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68acd2e6-6987-405c-a7e4-1ab2fd792146",
   "metadata": {},
   "outputs": [],
   "source": [
    "td=list(chimeras.keys())\n",
    "with mp.Pool(40) as pool:\n",
    "    results=pool.map(pick_secondaries,td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6fcca2c-ab91-4b1d-abc9-4a676670071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_chimera_map={x:y for x,y in zip(td,results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c7a9802-b83f-47df-8823-d903cee7243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save dictionary representation of filtered chimeras output\n",
    "file_path = 'outputs/secondary_chimera_selection_for_trees.pickle'\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(secondary_chimera_map,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ceda00d7-6d78-4236-b342-ba9f5fb8e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'outputs/secondary_chimera_selection_for_trees.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    secondary_chimera_map=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896d5c01-8b7a-4a0d-9f9a-1af94d35ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p outputs/phylogenetic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "538ea31e-6a60-4cf6-9f8f-915a4e6153bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "representatives=SeqIO.to_dict(SeqIO.parse('outputs/split_intervals.fasta', 'fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e67eb0a-f4bb-435d-a000-d4b948ed6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load arthropod protein accessions from NR\n",
    "ar_accessions=pd.read_csv('outputs/arthropoda.accessions',sep='\\t',header=None)\n",
    "ar_accessions=set(ar_accessions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4a6d982-7705-4b5d-9dd7-aaded6f2b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "#load fasta w/ all arthropod queries\n",
    "all_seqs = SeqIO.to_dict(SeqIO.parse('outputs/all_arthropod_concatenated_proteins.fa', 'fasta'))\n",
    "##add a secondary chimera pcr'd in the first pipeline iteration that is now suppressed in the latest a. albopictus annotation\n",
    "a2=SeqIO.to_dict(SeqIO.parse('outputs/suppressed_aedes_albopictus.fa', 'fasta'))\n",
    "all_seqs=all_seqs|a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41720cce-9591-43fc-8d32-c9e490affbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##these methods fill in taxonomic information in a dataframe of blast/hmmer hits to be used for phylogenetic inference\n",
    "def _init_ncbi():\n",
    "    \"\"\"Each worker process gets its own NCBITaxa instance.\"\"\"\n",
    "    global ncbi\n",
    "    ncbi = NCBITaxa()                  # downloads DB on first run\n",
    "\n",
    "\n",
    "def tax_info_list(binomial: str):\n",
    "    \"\"\"\n",
    "    Return [taxid, kingdom, phylum, class, order, lineage] for a species binomial,\n",
    "    or [nan, nan, nan, nan] if anything goes wrong.\n",
    "    \n",
    "    \"\"\"\n",
    "    _init_ncbi()\n",
    "    try:\n",
    "        taxid = ncbi.get_name_translator([binomial])[binomial][0]\n",
    "\n",
    "        lineage = ncbi.get_lineage(taxid)\n",
    "        ranks   = ncbi.get_rank(lineage)\n",
    "        names   = ncbi.get_taxid_translator(lineage)\n",
    "\n",
    "        kingdom = next((names[t] for t in lineage if ranks[t] == \"kingdom\"), np.nan)\n",
    "        phylum  = next((names[t] for t in lineage if ranks[t] == \"phylum\"),  np.nan)\n",
    "        classl  = next((names[t] for t in lineage if ranks[t] == \"class\"),  np.nan)\n",
    "        order  = next((names[t] for t in lineage if ranks[t] == \"order\"),  np.nan)\n",
    "        lineage = str([names[t] for t in lineage])\n",
    "\n",
    "        return [int(taxid), kingdom, phylum,classl,order,lineage]\n",
    "\n",
    "    except Exception:\n",
    "        return [np.nan, np.nan, np.nan,np.nan]\n",
    "\n",
    "\n",
    "# ---------- 2.  Annotate an entire DataFrame --------------------------------\n",
    "def add_taxonomy_columns(df, n_cores=40):\n",
    "    \"\"\"\n",
    "    Enrich *df* (must contain a 'species' column) with three new columns:\n",
    "       'taxid', 'kingdom', 'phylum'.\n",
    "    \"\"\"\n",
    "    with mp.Pool(processes=n_cores, initializer=_init_ncbi) as pool:\n",
    "        results = pool.map(tax_info_list, df[\"species\"].tolist())\n",
    "\n",
    "    # Split the list-of-lists into columns, preserving the original index\n",
    "    df[[\"taxid\", \"kingdom\", \"phylum\",\"class\",\"order\",'lineage']] = pd.DataFrame(results, index=df.index)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d22d2ae4-a1a4-4abe-bdfe-5f6d54370a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "runs BLAST+ blastdb_cmbdb to return a sliced protein sequence\n",
    "parameter: (protein accession, start coordinate, stop coordinate, indexed blast db path)\n",
    "\"\"\"\n",
    "def _fetch_slice(task):\n",
    "    acc, start, stop, blast_db = task\n",
    "    cmd = [\n",
    "        'blastdbcmd',\n",
    "        '-db', blast_db,\n",
    "        '-entry', acc,\n",
    "        '-range', f'{start}-{stop}',\n",
    "        '-outfmt', '%s'\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if result.returncode != 0 or not result.stdout.strip():\n",
    "        \n",
    "        return None\n",
    "    seq = result.stdout.replace('\\n', '')\n",
    "    return f'>{acc};({start},{stop})\\n{seq}\\n'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "accepts a data frame with hmmer or blast hits (specified by t)\n",
    "uses coordinates in the df to write slices of each protein to \"out_fasta\"\n",
    "parallelized with mp.pool\n",
    "\"\"\"\n",
    "def slice_proteins_to_fasta(df, out_fasta, blast_db='../dbs/nr', t='hmmer',workers=40):\n",
    "    if t=='blast':\n",
    "         tasks = [\n",
    "            (acc, int(start), int(stop), str(blast_db))\n",
    "            for acc, start, stop in df[['target_name', 'envfrom', 'envto']].itertuples(index=False)\n",
    "        ]\n",
    "    else:\n",
    "        tasks = [\n",
    "            (acc, int(start), int(stop), str(blast_db))\n",
    "            for acc, start, stop in df[['sseqid', 'sstart', 'send']].itertuples(index=False)\n",
    "        ]\n",
    "    with mp.Pool(processes=workers) as pool, open(out_fasta, 'a') as fout:\n",
    "        for fasta_record in pool.imap_unordered(_fetch_slice, tasks):\n",
    "            if fasta_record:\n",
    "                fout.write(fasta_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "388a8821-a977-4c5c-ab24-344ecbf7fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "takes an interval name and writes a fasta with hmmsearch-aligned intervals\n",
    "from secondary chimeras, non-chimeric arthropod sequences, and non-arthropod sequences\n",
    "selects a single sequence per ncbi taxid in each of the above categories\n",
    "calls a script to execute MUSCLE, trimAl and IQ-tree for phylogenetic inference\n",
    "\"\"\"\n",
    "def get_phylogen_dataset_hmmer(interv):\n",
    "    \n",
    "    ch=\";\".join(interv.split(\";\")[0:2])\n",
    "    os.makedirs(f'outputs/phylogenetic_dataset/{ch}', exist_ok=True)\n",
    "    os.makedirs(f'outputs/phylogenetic_dataset/{ch}/{interv}', exist_ok=True)\n",
    "\n",
    " \n",
    "    ##load_secondary chimeras \n",
    "    f=open(f'outputs/phylogenetic_dataset/{ch}/{interv}/all_sequences.fa','w')\n",
    "    sec=pd.read_csv(f\"outputs/hmmsearch_v_arthropod/{interv}.tsv\", sep=\"\\t\")\n",
    "    sec=sec.loc[sec.groupby('target_name')['domain_score'].idxmax(),:]\n",
    "    sec=sec[sec['target_name'].isin(secondary_chimera_map[ch])]\n",
    "    sec['secondary']=True\n",
    "    if sec.shape[0]>0:\n",
    "        \n",
    "        for index, row in sec.iterrows():\n",
    "            name=row.target_name\n",
    "            start=row.envfrom\n",
    "            stop=row.envto\n",
    "            seq=str(all_seqs[name].seq)[start-1:stop]\n",
    "            name=name+\";\"+str((start,stop)).replace(\" \",\"\")\n",
    "            f.write(f'>{name}\\n')\n",
    "            f.write(f'{seq}\\n')\n",
    "    ## add representative primary chimera to fasta and df\n",
    "    f.write(f\">{interv}\\n\")\n",
    "    s=str(representatives[interv].seq)\n",
    "    f.write(f\"{s}\\n\")\n",
    "    \n",
    "    n=sec.shape[0]\n",
    "    sec.loc[n,:]='primary_chimera'\n",
    "    sec.loc[n,'target_name']=ch\n",
    "    sec.loc[n,['envfrom','envto']]=ast.literal_eval(interv.split(\"_\")[-1])\n",
    "    sec.loc[n,'species']=dftax.loc[ch.split(\";\")[0],'Organism Name']\n",
    "    sec=add_taxonomy_columns(sec)\n",
    "    \n",
    "\n",
    "    ##load other arthropod_hits\n",
    "    arth_path = Path(f\"outputs/hmmsearch_v_arthropod/{interv}.tsv\")\n",
    "    \n",
    "    arth = pd.read_csv(arth_path, sep=\"\\t\",nrows=20000)\n",
    "    arth = arth[(arth['i-Evalue']<1e-2)]\n",
    "    arth=arth[~arth['target_name'].isin(secondary_chimera_adjacency_list[ch])]\n",
    "    arth=arth[arth['target_name']!=ch]\n",
    "   ##add non-secondary arthropod hits, 1/taxid\n",
    "    if arth.shape[0]>0:\n",
    "        arth=add_taxonomy_columns(arth)\n",
    "        idx = arth.groupby('taxid')['domain_score'].idxmax()   # index of winners by max domain_score/taxid\n",
    "        arth = arth.loc[idx].reset_index(drop=True)        # the filtered DataFrame\n",
    "        arth= arth[arth.taxid.astype('str')!='nan']\n",
    "        for index, row in arth.iterrows():\n",
    "            name=row.target_name\n",
    "            start=row.envfrom\n",
    "            stop=row.envto\n",
    "            seq=str(all_seqs[name].seq)[start-1:stop]\n",
    "            name=name+\";\"+str((start,stop)).replace(\" \",\"\")\n",
    "            f.write(f'>{name}\\n')\n",
    "            f.write(f'{seq}\\n')\n",
    "    f.close()\n",
    "   \n",
    "    ##Load non-arthropod hits\n",
    "    non_arth=pd.read_csv(f\"outputs/hmmsearch_v_nr/{interv}.tsv\", sep=\"\\t\",nrows=20000)\n",
    "    non_arth=non_arth[~non_arth.target_name.isin(ar_accessions)]\n",
    "    non_arth=non_arth[non_arth['i-Evalue']<1e-2]\n",
    "    non_arth=non_arth.sort_values('i-Evalue')\n",
    "    \n",
    "    non_arth=non_arth.iloc[0:20000,:]\n",
    "    if non_arth.shape[0]>0:\n",
    "        non_arth=add_taxonomy_columns(non_arth)\n",
    "        ##filter out synthetic sequence hits and double-check arthropod filtering\n",
    "        non_arth=non_arth[(non_arth.taxid.astype('str')!='nan')&(non_arth.taxid!=32630)&(non_arth.phylum!='Arthropoda')] \n",
    "        idx = non_arth.groupby('taxid')['domain_score'].idxmax()   \n",
    "        non_arth = non_arth.loc[idx].reset_index(drop=True)     \n",
    "        \n",
    "        ##add up to 500 non-metazoan hits\n",
    "        non_meta=non_arth[~non_arth.kingdom.astype(str).str.contains('Metazoa')]\n",
    "        non_meta=non_meta.iloc[0:500,:]\n",
    "        slice_proteins_to_fasta(non_meta,f'outputs/phylogenetic_dataset/{ch}/{interv}/all_sequences.fa',t='blast')\n",
    "\n",
    "        ## add up to 500 non-arthropod metazoan hits \n",
    "        non_arth_meta=non_arth[~(non_arth.phylum.astype(str).str.contains('Arthropoda'))&(non_arth.kingdom=='Metazoa')]\n",
    "        non_arth_meta=non_arth_meta.iloc[0:500,:]\n",
    "        slice_proteins_to_fasta(non_arth_meta,f'outputs/phylogenetic_dataset/{ch}/{interv}/all_sequences.fa',t='blast')\n",
    "\n",
    "    combined=pd.concat([sec,arth,non_arth_meta,non_meta])\n",
    "    combined.to_csv(f'outputs/phylogenetic_dataset/{ch}/{interv}/combined_sequences_data.tsv',sep='\\t')\n",
    "    if combined.shape[0]<500:\n",
    "        !sbatch scripts/align_iq_pipe.sh \"$ch\" \"$interv\"\n",
    "    else:\n",
    "        !sbatch scripts/align_iq_pipe_long.sh \"$ch\" \"$interv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a4168a6-2782-4ab0-b4c9-bb3a4ffc4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "takes an interval name and writes a fasta with DIAMOND BLASTP-aligned intervals\n",
    "from secondary chimeras, non-chimeric arthropod sequences, and non-arthropod sequences\n",
    "selects a single sequence per ncbi taxid in each of the above categories\n",
    "calls a script to execute MUSCLE, trimAl and IQ-tree for phylogenetic inference\n",
    "\"\"\"\n",
    "def get_phylogen_dataset_blast(interv):\n",
    "    \n",
    "    ch=\";\".join(interv.split(\";\")[0:2])\n",
    "    os.makedirs(f'outputs/phylogenetic_dataset/{ch}', exist_ok=True)\n",
    "    os.makedirs(f'outputs/phylogenetic_dataset/{ch}/{interv}', exist_ok=True)\n",
    "\n",
    " \n",
    "    ##load_secondary chimeras \n",
    "    f=open(f'outputs/phylogenetic_dataset/{ch}/{interv}/all_sequences.fa','w')\n",
    "    \n",
    "    sec=pd.read_csv(f\"outputs/round2_diamond_v_arthropod_output_split/{interv}.tsv\", sep=\"\\t\")\n",
    "    sec=sec.loc[sec.groupby('sseqid')['bitscore'].idxmax(),:]\n",
    "    sec=sec[sec['sseqid'].isin(secondary_chimera_map[ch])]\n",
    "    sec['species']=[dftax.loc[x.split(\";\")[0],'Organism Name'] for x in sec['sseqid']]\n",
    "    sec['secondary']=True\n",
    "    if sec.shape[0]>0:\n",
    "        for index, row in sec.iterrows():\n",
    "            name=row.sseqid\n",
    "            start=row.sstart\n",
    "            stop=row.send\n",
    "            seq=str(all_seqs[name].seq)[start-1:stop]\n",
    "            name=name+\";\"+str((start,stop)).replace(\" \",\"\")\n",
    "            f.write(f'>{name}\\n')\n",
    "            f.write(f'{seq}\\n')\n",
    "    \n",
    "    ### add representative primary chimera to fasta and df\n",
    "    f.write(f\">{interv}\\n\")\n",
    "    s=str(representatives[interv].seq)\n",
    "    f.write(f\"{s}\\n\")\n",
    "    \n",
    "    n=sec.shape[0]\n",
    "    sec.loc[n,:]='primary_chimera'\n",
    "    sec.loc[n,'sseqid']=ch\n",
    "    sec.loc[n,['sstart','send']]=ast.literal_eval(interv.split(\"_\")[-1])\n",
    "    sec.loc[n,'species']=dftax.loc[ch.split(\";\")[0],'Organism Name']\n",
    "    sec=add_taxonomy_columns(sec)\n",
    "    \n",
    "\n",
    "    ##load other arthropod_hits\n",
    "    arth_path = Path(f\"outputs/round2_diamond_v_arthropod_output_split/{interv}.tsv\")\n",
    "    \n",
    "    arth = pd.read_csv(arth_path, sep=\"\\t\",nrows=20000)\n",
    "    arth['species']=[dftax.loc[x.split(\";\")[0],'Organism Name'] for x in arth['sseqid']]\n",
    "    arth = arth[(arth['evalue']<1e-2)]\n",
    "    arth=arth[~arth['sseqid'].isin(secondary_chimera_adjacency_list[ch])]\n",
    "    arth=arth[arth['sseqid']!=ch]\n",
    "   \n",
    "    if arth.shape[0]>0:\n",
    "        arth=add_taxonomy_columns(arth)\n",
    "        idx = arth.groupby('taxid')['bitscore'].idxmax()   # index of winners by max domain_score/taxid\n",
    "        arth = arth.loc[idx].reset_index(drop=True)        # the filtered DataFrame\n",
    "        arth= arth[arth.taxid.astype('str')!='nan']\n",
    "        for index, row in arth.iterrows():\n",
    "            name=row.sseqid\n",
    "            start=row.sstart\n",
    "            stop=row.send\n",
    "            seq=str(all_seqs[name].seq)[start-1:stop]\n",
    "            name=name+\";\"+str((start,stop)).replace(\" \",\"\")\n",
    "            f.write(f'>{name}\\n')\n",
    "            f.write(f'{seq}\\n')\n",
    "\n",
    "    f.close()\n",
    "    ##Load non-arthropod hits\n",
    "    non_arth=pd.read_csv(f\"outputs/round2_diamond_output_split/{interv}.tsv\", sep=\"\\t\",nrows=20000)\n",
    "    non_arth=non_arth[~non_arth.sphylums.astype(str).str.contains('Arthropoda')]\n",
    "    non_arth['species']=non_arth['sscinames']\n",
    "    non_arth=non_arth.drop('sscinames',axis=1)\n",
    "    non_arth=non_arth[non_arth['evalue']<1e-2]\n",
    "    non_arth=non_arth.sort_values('evalue')\n",
    "    \n",
    "    non_arth=non_arth.iloc[0:20000,:]\n",
    "    if non_arth.shape[0]>0:\n",
    "        non_arth=add_taxonomy_columns(non_arth)\n",
    "        ##filter out synthetic sequence hits\n",
    "        non_arth=non_arth[(non_arth.staxids.astype('str')!='nan')&(non_arth.staxids!=32630)]\n",
    "        idx = non_arth.groupby('taxid')['bitscore'].idxmax()   # index of winners by max domain_score/taxid\n",
    "        non_arth = non_arth.loc[idx].reset_index(drop=True)        # the filtered DataFrame\n",
    "        ##add up to 500 non-metazoan hits\n",
    "        non_meta=non_arth[~non_arth.skingdoms.astype(str).str.contains('Metazoa')]\n",
    "        non_meta=non_meta.iloc[0:500,:]\n",
    "        slice_proteins_to_fasta(non_meta,f'outputs/phylogenetic_dataset/{ch}/{interv}/all_sequences.fa')\n",
    "\n",
    "        ##add up to 500 non-arthropod metazoan hits \n",
    "        non_arth_meta=non_arth[(non_arth.skingdoms=='Metazoa')]\n",
    "        non_arth_meta=non_arth_meta.iloc[0:500,:]\n",
    "        slice_proteins_to_fasta(non_arth_meta,f'outputs/phylogenetic_dataset/{ch}/{interv}/all_sequences.fa')\n",
    "\n",
    "    combined=pd.concat([sec,arth,non_arth_meta,non_meta])\n",
    "    combined.to_csv(f'outputs/phylogenetic_dataset/{ch}/{interv}/combined_sequences_data.tsv',sep='\\t')\n",
    "    print(combined.shape[0])\n",
    "    if combined.shape[0]<500:\n",
    "        !sbatch scripts/align_iq_pipe.sh \"$ch\" \"$interv\"\n",
    "    else:\n",
    "        !sbatch scripts/align_iq_pipe_long.sh \"$ch\" \"$interv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c996a15c-f29c-4088-9959-8157434a0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "##run phylogenetic inference on HGT intervals with hmmer intervals\n",
    "td=[x for x in intervals if 'HGT' in x and x in hmmer_hits ]\n",
    "with mp.Pool(40) as pool:\n",
    "    results=pool.map(get_phylogen_dataset_hmmer,td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d4ff4-6856-448b-ba77-3291ad1e0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##run phylogenetic inference on HGT intervals with blast intervals\n",
    "td=[x for x in intervals if 'HGT' in x and x in blast_hits ]\n",
    "with mp.Pool(40) as pool:\n",
    "    results=pool.map(get_phylogen_dataset_blast,td)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61131ab5-1461-4ae6-91db-f3a45d65d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load manual inspection results for HGT trees (note--only metazoan intervals from HGT-confirmed trees were inspected)\n",
    "\n",
    "hgt=pd.read_csv(\"Tree_manual_inspection_HGT.tsv\",sep='\\t',index_col=0)\n",
    "hgt=hgt[hgt.Tree_annot=='Yes']\n",
    "confirmed_hgt=[\";\".join(x.split(\";\")[0:2]) for x in hgt.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bedc15bc-7622-40a0-aa61-134325b332ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##run phylogenetic inference on Metazoan intervals from chimeras with confirmed HGT intervals\n",
    "td=[x for x in intervals if 'Meta' in x and x.split(\";\")[1] in confirmed_hgt and x in hmmer_hits]\n",
    "with mp.Pool(40) as pool:\n",
    "    results=pool.map(get_phylogen_dataset_hmmer,td)\n",
    "\n",
    "td=[x for x in intervals if 'Meta' in x and x.split(\";\")[1] in confirmed_hgt and x in blast_hits]\n",
    "with mp.Pool(40) as pool:\n",
    "    results=pool.map(get_phylogen_dataset_blast,td)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rishabh]",
   "language": "python",
   "name": "conda-env-.conda-rishabh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

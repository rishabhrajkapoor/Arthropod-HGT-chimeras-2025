{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "317e37fe-86be-42bb-862d-697bc51e9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import subprocess\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ab48b-55f8-42a1-baf9-cd79a80dbe79",
   "metadata": {},
   "source": [
    "## Correct fasta headers for all arthropod protein fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c2a4dd-3567-4528-8b51-abdab830bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##load fasta of all arthropod proteins in original search database\n",
    "all_seqs = SeqIO.to_dict(SeqIO.parse('outputs/all_arthropod_concatenated_proteins.fa', 'fasta'))\n",
    "\n",
    "## correct the double appending of refseq genome accessions to the arthropod concatenated fasta\n",
    "def _rename_kv(kv):\n",
    "    orig_id, rec = kv\n",
    "    new_id = \";\".join(orig_id.split(\";\")[-2:])\n",
    "    rec.id = rec.name = rec.description = new_id\n",
    "    return new_id, rec\n",
    "\n",
    "with mp.Pool(20) as pool:\n",
    "    renamed_pairs = pool.map(_rename_kv, all_seqs.items())\n",
    "\n",
    "new_seqs = dict(renamed_pairs)\n",
    "with open(\"outputs/all_arthropod_concatenated_proteins.fa\", \"w\") as handle:\n",
    "    SeqIO.write(new_seqs.values(), handle, \"fasta\")\n",
    "    \n",
    "##correct blast outputs and add headers   \n",
    "for n in os.listdir(\"outputs/round2_diamond_v_arthropod_output_split\"):\n",
    "    arth=pd.read_csv(f\"outputs/round2_diamond_v_arthropod_output_split/{n}\",sep=\"\\t\", names=\"qseqid sseqid stitle pident length mismatch gapopen qstart qend sstart send evalue bitscore\".split(\" \"))\n",
    "    arth.sseqid=[\";\".join(x.split(\";\")[-2:]) for x in arth.sseqid]\n",
    "    arth.stitle=[\";\".join(x.split(\";\")[-2:]) for x in arth.sseqid]\n",
    "    arth.to_csv(f\"outputs/round2_diamond_v_arthropod_output_split/{n}\",sep=\"\\t\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6b079-b1a6-412e-919a-8e1fd6efcd44",
   "metadata": {},
   "source": [
    "## Build profile HMMs for each interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "087c0e0a-ce85-47f1-a00a-30bfde0cc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load filtered round2 blast chimeras\n",
    "import pickle\n",
    "file_path = 'outputs/round2_chimera_intervals.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    chimeras=pickle.load(file)\n",
    "##append to intervals\n",
    "intervals=[]\n",
    "for c in chimeras:\n",
    "    for i in chimeras[c]:\n",
    "        intervals.append(c+\";\"+chimeras[c][i]+\"_\"+str(i).replace(\" \",\"\"))\n",
    "##load fasta of separated chimera intervals\n",
    "interval_queries=SeqIO.to_dict(SeqIO.parse('outputs/split_intervals.fasta', 'fasta'))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f16dc5c-8313-4aa1-9ce2-74a93fb5458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#takes a sequence accession n[0]\n",
    "#write to output fasta (n[1]).\n",
    "def get_fasta(n):\n",
    "    n1=n[0]\n",
    "    n2=n[1]\n",
    "    f=open(n2,'a')\n",
    "    f.write(f\">{n1}\\n\")\n",
    "    s=str(all_seqs[n1].seq)\n",
    "    f.write(s+\"\\n\")\n",
    "    f.close()\n",
    "    return n\n",
    "\n",
    "#write a copy of fasta_file (str) with all intervals in blast df to output_file (str)\n",
    "def copy_fasta_with_substr(fasta_file, df, output_file):\n",
    "    with open(output_file, \"w\") as out_handle:\n",
    "        for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            seq_name = seq_record.id\n",
    "       \n",
    "            if seq_name in df[\"sseqid\"].values:\n",
    "                sub_df = df[df[\"sseqid\"] == seq_name]\n",
    "                for _, row in sub_df.iterrows():\n",
    "                    sstart = row[\"sstart\"]\n",
    "                    send = row[\"send\"]\n",
    "                    subseq = seq_record.seq[sstart-1:send]\n",
    "                    subseq_name = f\"{seq_name}_{sstart}_{send}\"\n",
    "                    subseq_record = seq_record\n",
    "                    subseq_record.id = subseq_name\n",
    "                    subseq_record.description = \"\"\n",
    "                    subseq_record.seq = subseq\n",
    "                    SeqIO.write(subseq_record, out_handle, \"fasta\")\n",
    "                    \n",
    "#run MUSCLE to get MSA for multi-seq fasta n (str)\n",
    "def get_muscle(n):\n",
    "    protein=\";\".join(n.split(\";\")[0:2])\n",
    "    a=subprocess.run([\"sh\",\"scripts/hmmbuild_muscle.sh\", f\"outputs/hmmbuild/{protein}/{n}\"])\n",
    "    \n",
    "#run hmm_build to obtain HMMER profile HMM for MSA for query n     \n",
    "def get_hmm_profile(n):\n",
    "    protein=\";\".join(n.split(\";\")[0:2])\n",
    "    a=subprocess.run([\"sh\",\"scripts/hmmbuild.sh\", f\"outputs/hmmbuild/{protein}/{n}\"])\n",
    "    \n",
    "#takes the name of an interval \n",
    "#writes sequences of arthropod blast hits with >30% coverge of interval and e-value < min e-value of non-arthropod hits to fasta \n",
    "#runs MUSCLE to make a MSA, then builds profile HMM\n",
    "def write_fastas_run_hmm(n):\n",
    "    try:\n",
    "        ##protein accession prefix (genome;protein)\n",
    "        protein=\";\".join(n.split(\";\")[0:2])\n",
    "\n",
    "        ##load non-arthropod hits\n",
    "        df=pd.read_csv(f\"outputs/round2_diamond_output_split/{n}.tsv\",sep=\"\\t\", names=\"qseqid sseqid stitle staxids sscinames sphylums skingdoms pident length mismatch gapopen qstart qend sstart send evalue bitscore\".split(\" \"))\n",
    "        df[\"cover\"]=(np.array(df.qend)-np.array(df.qstart)+1)/len(interval_queries[n].seq)\n",
    "        df=df[df.cover>.30]\n",
    "        ##exclude synthetic seqs\n",
    "        df=df[df.staxids.astype(str)!=\"32630\"]\n",
    "        non_arth=df[~df.astype(str).sphylums.str.contains(\"Arthropoda\")]\n",
    "\n",
    "        ##Load arthropod hits from diamond blast to arthropod database\n",
    "        arth=pd.read_csv(f\"outputs/round2_diamond_v_arthropod_output_split/{n}.tsv\",sep=\"\\t\")\n",
    "        arth[\"cover\"]=(np.array(arth.qend)-np.array(arth.qstart)+1)/len(interval_queries[n].seq)\n",
    "        arth=arth[arth.cover>.30]\n",
    "\n",
    "        ##arthropod seqs with a greater bitscore than the top non-arthropod sequence\n",
    "        arth=arth[arth.bitscore>=non_arth.bitscore.max()]\n",
    "        try:\n",
    "            os.mkdir(f\"outputs/hmmbuild/{protein}\")\n",
    "        except:\n",
    "            p=1\n",
    "        try:\n",
    "            os.mkdir(f\"outputs/hmmbuild/{protein}/{n}\")\n",
    "        except:\n",
    "            p=1\n",
    "\n",
    "        for f in [(x,f\"outputs/hmmbuild/{protein}/{n}/seq.fasta\") for x in set(arth.sseqid)]:\n",
    "            get_fasta(f)\n",
    "        with open(f\"outputs/hmmbuild/{protein}/{n}/unique_seq.fasta\", \"w\") as outfile:\n",
    "        # Run awk command to remove redundant sequences\n",
    "            subprocess.run([\"awk\", \"-f\", \"scripts/remove_redundant_seqs.awk\", f\"outputs/hmmbuild/{protein}/{n}/seq.fasta\"], stdout=outfile)\n",
    "        # extract portion of outputs alignign with query interval\n",
    "        copy_fasta_with_substr(f\"outputs/hmmbuild/{protein}/{n}/unique_seq.fasta\",arth,f\"outputs/hmmbuild/{protein}/{n}/sub_seq.fasta\")\n",
    "        get_muscle(n)\n",
    "        get_hmm_profile(n)\n",
    "        del arth, non_arth, df\n",
    "    except:\n",
    "        f=open('failed_hmmbuild.txt','a')\n",
    "        f.write(n+\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c10144-2769-4218-b3ff-05b08f4cf0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('outputs/hmmbuild')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e8a114-6e5a-43d9-a753-6dad9e51da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "td=intervals\n",
    "##run the write hmm script in parallel on all intervals\n",
    "with mp.Pool(20) as pool:\n",
    "    r = pool.map(write_fastas_run_hmm, td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec0b8f8b-cf23-4d57-af89-deb175aa511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##collect all intervals with completed MSAs\n",
    "\n",
    "base_dir=Path(\"outputs/hmmbuild\")\n",
    "level3_dirs = [\n",
    "    p for p in base_dir.glob(\"*/*\")    # 3 segments under base_dir\n",
    "    if p.is_dir()\n",
    "]\n",
    "done=set()\n",
    "lens=[]\n",
    "for d in level3_dirs:\n",
    "    if \"sub_seq.hmm\" in os.listdir(str(d))  :\n",
    "        done.add(str(d).split(\"/\")[-1])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f308fcca-3faf-444b-9555-725df395c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##collect intervals where the MSAs fail to run within 20 hours, run with muscle super5 \n",
    "td=list(set(done)-set(td))\n",
    "def get_muscle(n):\n",
    "    protein=\";\".join(n.split(\";\")[0:2])\n",
    "    a=subprocess.run([\"sh\",\"scripts/hmmbuild_muscle_super5.sh\", f\"outputs/hmmbuild/{protein}/{n}\"])\n",
    "\n",
    "with mp.Pool(20) as pool:\n",
    "    r = pool.map(write_fastas_run_hmm, td)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d673bbc-66e4-4ac1-8e09-b1c80caa2c81",
   "metadata": {},
   "source": [
    "## Concatenate profile hmms and submit to hmmsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7a33aa-d3e5-4211-af4b-036f13698965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##collect all hmms that have successfully run\n",
    "base_dir=Path(\"outputs/hmmbuild\")\n",
    "level3_dirs = [\n",
    "    p for p in base_dir.glob(\"*/*\")    # 3 segments under base_dir\n",
    "    if p.is_dir()\n",
    "]\n",
    "done=set()\n",
    "lens=[]\n",
    "for d in level3_dirs:\n",
    "    if \"sub_seq.hmm\" in os.listdir(str(d))  :\n",
    "        done.add(str(d)+\"/sub_seq.hmm\")\n",
    "    \n",
    "a=\"~\".join(done)\n",
    "##concatenate all the hmms into a single file for hmmsearch agaisnt the arthropod-only database\n",
    "!sh scripts/concat_hmms.sh \"all_concatenated\".hmm \"$a\"\n",
    "        \n",
    "##make concatenated hmm files of at most 10 hmms to run in parallel against nr        \n",
    "def chunk(lst, size=10):\n",
    "    \"\"\"Yield successive `size`-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), size):\n",
    "        yield lst[i : i + size]\n",
    "\n",
    "chunks = list(chunk(list(done), 10))\n",
    "\n",
    "chunk_map={i:chunks[i] for i in range(len(chunks))}\n",
    "\n",
    "!mkdir concatenated_hmms\n",
    "for i in chunk_map:\n",
    "    j=\"~\".join(chunk_map[i])\n",
    "    ix=i+1\n",
    "    !scripts/concat_hmms.sh \"concatenated_hmms/$ix\".hmm \"$j\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ac8404-e34f-4a5b-befa-36caa67edc5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 13970905\n"
     ]
    }
   ],
   "source": [
    "##run hmmsearch vs nr on split concatenated profile hmms in parallel\n",
    "!mkdir hmmsearchout_concat\n",
    "!sbatch scripts/hmmsearch_array.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26a2356a-ea03-4ccb-81c0-074e32ea823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 14150084\n"
     ]
    }
   ],
   "source": [
    "##run hmmsearch vs custom arthropod proteome \n",
    "!sbatch scripts/hmmsearch_vs_arthropoda.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc57ae64-aaab-4378-ae1c-21846a68ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##split hmmsearch outputs from arthropoda search when done \n",
    "!sh scripts/split_hmmer_csv.sh hmmsearch_v_arthropod_db.domtblout outputs/hmmsearch_v_arthropod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c335094-d630-4c17-8e94-38f299d63562",
   "metadata": {},
   "outputs": [],
   "source": [
    "##split hmmsearch outputs from NR search when done \n",
    "def split_hmmer_result(x):\n",
    "    !sh scripts/split_hmmer_csv.sh hmmsearchout_concat/\"$x\" \"outputs/hmmsearch_v_nr\"\n",
    "td=[x for x in os.listdir('hmmsearchout_concat') if '.domtblout' in x]\n",
    "import multiprocessing as mp\n",
    "with mp.Pool(20) as pool:\n",
    "    result = pool.map(split_hmmer_result, td)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f29541-94a4-402a-8f89-462a6bd49c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rishabh]",
   "language": "python",
   "name": "conda-env-.conda-rishabh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7c80f9-10c3-4113-8366-6eccd29af6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "import subprocess\n",
    "import ast\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97722b6d-bf2b-4d27-ae80-7bd52530899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load filtered round2 blast chimeras\n",
    "import pickle\n",
    "file_path = 'outputs/transposon_ankyrin_filtered_round2_chimera_intervals.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    chimeras=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4973f6-11c0-483a-a0ef-017dcbf17baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##append to intervals\n",
    "intervals=[]\n",
    "for c in chimeras:\n",
    "    for i in chimeras[c]:\n",
    "        intervals.append(c+\";\"+chimeras[c][i]+\"_\"+str(i).replace(\" \",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "765b1583-b0f2-459f-b20d-09db830981f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chimeras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2029cb70-e0e8-4565-b59f-e9432c56da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "#load fasta w/ all arthropod queries\n",
    "all_seqs = SeqIO.to_dict(SeqIO.parse('outputs/all_arthropod_concatenated_proteins.fa', 'fasta'))\n",
    "##add a secondary chimera pcr'd in the first pipeline iteration that is now suppressed in the latest a. albopictus annotation\n",
    "a2=SeqIO.to_dict(SeqIO.parse('outputs/suppressed_aedes_albopictus.fa', 'fasta'))\n",
    "all_seqs=all_seqs|a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f57060e9-cad8-4f46-b6ac-58b582687632",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load a dataframe of genome taxids from genome accessions\n",
    "df1=pd.read_csv('Data/genbank_genomes_4_22_2025.tsv',sep='\\t')\n",
    "df2=pd.read_csv('Data/refseq_genomes_scaffold_plus_4_19_2025.tsv',sep='\\t')\n",
    "dftax=pd.concat([df1,df2]).set_index('Assembly Accession')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c83a68-cf24-4b61-b85c-99dad7c859c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process hmmsearch output tsvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6059a22a-b0e4-419c-974f-05b202a7ca07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "cols = [\n",
    "    \"target_name\",\"tlen\",\"query name\",\"qlen\",\"E-value\",\"overall_score\",\n",
    "    \"overall_bias\",\"#\",\"of\",\"c-Evalue\",\"i-Evalue\",\"domain_score\",\"bias\",\n",
    "    \"hmmfrom\",\"hmmto\",\"alifrom\",\"alito\",\"envfrom\",\"envto\",\"acc\",\n",
    "    \"description of target\",\"species\"\n",
    "]\n",
    "\n",
    "def process_interval(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    Process hmmsearch hits for an interval: add a header, remove synthetic hits, \n",
    "    add taxid and species info to arthropod hits\n",
    "    \"\"\"\n",
    "    \n",
    "    arth_path = Path(f\"outputs/hmmsearch_v_arthropod/{x}.tsv\")\n",
    "    nr_path   = Path(f\"outputs/hmmsearch_v_nr/{x}.tsv\")\n",
    "    if 'species' not in open(arth_path,'r').readline():\n",
    "        \n",
    "        # original raw files: no header\n",
    "        arth = pd.read_csv(arth_path, sep=\"\\t\", header=None)\n",
    "        # drop unused cols\n",
    "        arth = arth.drop([1,4], axis=1)\n",
    "        # assign column names\n",
    "        arth.columns = cols\n",
    "        # annotate arthropod hits with species & taxid\n",
    "        arth['species'] = [ dftax.loc[name.split(\";\")[0], 'Organism Name'] for name in arth['target_name'] ]\n",
    "        arth['taxid']   = [ dftax.loc[name.split(\";\")[0], 'Organism Taxonomic ID'] for name in arth['target_name'] ]\n",
    "        arth = arth.drop('description of target', axis=1)\n",
    "        arth.to_csv(arth_path, sep=\"\\t\", index=False)\n",
    "        del arth\n",
    "    \n",
    "    if 'species' not in open(nr_path,'r').readline():\n",
    "        nr   = pd.read_csv(nr_path,   sep=\"\\t\", header=None)\n",
    "        nr   = nr.drop(  [1,4], axis=1)\n",
    "        nr.columns = cols\n",
    "        nr = nr[~nr['species'].astype(str).str.contains(\"synthetic\")]\n",
    "        nr.to_csv(nr_path,   sep=\"\\t\", index=False)\n",
    "        del nr \n",
    "    return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "034661ff-4f1c-4cb7-952b-776f95472a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(40) as pool:\n",
    "    results=pool.map(process_interval,intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b66a60-4ed3-497f-93bb-0cd923d20867",
   "metadata": {},
   "source": [
    "## Extract arthropod secondary chimeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82ff8fb7-fb34-4e14-af0b-55bff9791cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load arthropod protein accessions from NR\n",
    "ar=pd.read_csv('outputs/arthropoda.accessions',sep='\\t',header=None)\n",
    "ar=set(ar[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bacff86-2d97-4ff5-85f5-e1e8194d586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##extract sequences that have no non-self blast-hits (bit-score>min(bit-score non-arthropod))\n",
    "##these are assumed to have 0 secondary chimeras  and blast hits instead of hmmsearch hits are used for phylogenetic dataset contstruction\n",
    "singleton_blast_hits=[]\n",
    "for chimera in chimeras:\n",
    "    ints=[x for x in intervals if chimera in x]\n",
    "    for x in ints:\n",
    "        a2=SeqIO.to_dict(SeqIO.parse(f'outputs/hmmbuild/{chimera}/{x}/sub_seq.fasta', 'fasta'))\n",
    "        if len(a2.keys())<=1:\n",
    "       \n",
    "            singleton_blast_hits.append(x)\n",
    "##chimeras that will be analyzed with diamond hits alone\n",
    "singleton_blast_proteins=set([\";\".join(x.split(';')[0:2]) for x in singleton_blast_hits])\n",
    "##chimeras with no diamond hit only intervals\n",
    "non_singleton_chimeras=set(chimeras.keys())-singleton_blast_proteins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a078929d-4833-4998-93d2-20522b7e4d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 807)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(singleton_blast_hits), len(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02970fff-969d-44bc-b635-2bbb0cff3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load a dataframe of genome taxids from genome accessions\n",
    "df1=pd.read_csv('Data/genbank_genomes_4_22_2025.tsv',sep='\\t')\n",
    "df2=pd.read_csv('Data/refseq_genomes_scaffold_plus_4_19_2025.tsv',sep='\\t')\n",
    "dftax=pd.concat([df1,df2]).set_index('Assembly Accession')\n",
    "dftax.loc['GCF_006496715.1',['Organism Name','Organism Taxonomic ID']]=['Aedes albopictus',7160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2809f65a-68b7-42a3-8b6d-80b7de54e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "takes an interval name, returns arhropod hmmsearch hits e-value<1e-2, along with coordinates\n",
    "\"\"\"\n",
    "def get_arthropod_hit_set(x,e_thresh=1e-4):\n",
    "    ##extract blast hits if hmm profile is not built from >1 sequence\n",
    "    if x in singleton_blast_hits:\n",
    "        arth_path = Path(f\"outputs/round2_diamond_v_arthropod_output_split/{x}.tsv\")\n",
    "        arth = pd.read_csv(arth_path, sep=\"\\t\")\n",
    "        arth['species']=[dftax.loc[x.split(\";\")[0],'Organism Name'] for x in arth['sseqid']]\n",
    "        arth = arth[(arth['evalue']<e_thresh)]\n",
    "        arth = arth.loc[arth.groupby('sseqid', as_index=True)['bitscore'].idxmax(),:]\n",
    "        # build the set of hit descriptors\n",
    "        hit_set = {row.sseqid:[(row.sstart,row.send),row.species]\n",
    "            for row in arth.itertuples()}\n",
    "    ## else extract hmmsearch hits\n",
    "    else:\n",
    "        arth_path = Path(f\"outputs/hmmsearch_v_arthropod/{x}.tsv\")\n",
    "        arth = pd.read_csv(arth_path, sep=\"\\t\")\n",
    "        ##exception for the aedes albopictus hit GCF_006496715.1;XP_029735553.1 added for chimera GCF_002204515.2;XP_021699539.1\n",
    "        if 'GCF_002204515.2;XP_021699539.1' in x:\n",
    "            arth = arth[(arth['i-Evalue']<e_thresh)|(arth.target_name=='GCF_006496715.1;XP_029735553.1')]\n",
    "        else:\n",
    "            arth = arth[(arth['i-Evalue']<e_thresh)]\n",
    "        arth = arth.loc[arth.groupby('target_name', as_index=True)['domain_score'].idxmax(),:]\n",
    "        # build the set of hit descriptors\n",
    "        hit_set = {row.target_name:[(row.envfrom,row.envto),row.species]\n",
    "            for row in arth.itertuples()}\n",
    "    del arth\n",
    "    return hit_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73da77e5-f036-4f26-a91a-2a09e837781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(40) as pool:\n",
    "    results=pool.map(get_arthropod_hit_set,intervals)\n",
    "##build a dictionary of intervals to their hits\n",
    "chimera_hits_dict={x:y for x,y in zip(intervals,results)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bdb062c-d21c-412f-ace7-85d806091d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get the set of simultaneous hits to all intervals of the primary chimera\n",
    "x: chimera accession\n",
    "\"\"\"\n",
    "def get_hit_intersection(x):\n",
    "\n",
    "    # build a list of all hit sets for all intervals in chimera x\n",
    "    sets_to_intersect = [\n",
    "        {g for g in chimera_hits_dict[inter] }\n",
    "        for inter in intervals\n",
    "        if x in inter\n",
    "    ]\n",
    "    return (set.intersection(*sets_to_intersect) if sets_to_intersect else set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92b9d7fb-601a-46ea-80cd-d3e4b6b1343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dictionary from chimera to all arthropod proteins that are simultaneous hits to all chimera intervals\n",
    "chimera_intersection={}\n",
    "for x in chimeras:\n",
    "    chimera_intersection[x]=get_hit_intersection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afbd1784-055b-4c1b-9951-554bf2565d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confirm_secondary_chimera_interval_order(ch):\n",
    "    \"\"\"\n",
    "    For a primary chimera accession (`ch`)—\n",
    "\n",
    "    • loop over every **HGT** interval in its linear order  \n",
    "    • pull the corresponding arthropod HMMer or blast coordinates in putative secondary chimeras (chimera_intersection dictionary)\n",
    "    • check that the flanking N-terminal and/or C-terminal\n",
    "      **metazoan** intervals are also present *in the right order*  in every putative secondary chimera\n",
    "        – up-stream Meta block must start before the HGT block  \n",
    "        – down-stream Meta block must start after the HGT block  \n",
    "        – any mutual overlap with the HGT block must be < 15 bp/aa\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of confirmed secondary chimeras\n",
    "    \"\"\"\n",
    "    # Interval-class mapping for this chimera, *in ascending order*\n",
    "    d = chimeras[ch]\n",
    "    intervals = list(d.keys())\n",
    "\n",
    "    # Start an empty results frame indexed by the chimera/intersection set\n",
    "    df = pd.DataFrame(index=list(chimera_intersection[ch]))\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Walk through the ordered intervals, keeping track of position (i)\n",
    "    # ---------------------------------------------------------------------\n",
    "    for i, interval in enumerate(intervals):\n",
    "\n",
    "        # Skip anything that isn't an HGT chunk\n",
    "        if d[interval] != 'HGT':\n",
    "            continue\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # (1)  Annotate HGT coordinates + species\n",
    "        # -------------------------------------------------------------\n",
    "        ints = str(interval).replace(\" \", \"\")\n",
    "        hgt_hit_set = get_arthropod_hit_set(f\"{ch};HGT_{ints}\")\n",
    "        for idx in set(df.index) & set(hgt_hit_set):\n",
    "         \n",
    "            df.loc[idx, 'hgt_start'] = hgt_hit_set[idx][0][0]\n",
    "            df.loc[idx, 'hgt_end']   = hgt_hit_set[idx][0][1]\n",
    "            df.loc[idx, 'species']   = hgt_hit_set[idx][1]\n",
    "            \n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # (2)  Check the *up-stream* Meta block (N-terminal side)\n",
    "        # -------------------------------------------------------------\n",
    "        if i > 0 and df.shape[0]>0 and d[intervals[i-1]] == 'Meta':\n",
    "            ints = str(intervals[i-1]).replace(\" \", \"\")\n",
    "            meta_hit_set = get_arthropod_hit_set(f\"{ch};Meta_{ints}\")\n",
    "\n",
    "            for idx in set(meta_hit_set) & set(df.index):\n",
    "                df.loc[idx, 'meta_up_start'] = meta_hit_set[idx][0][0]\n",
    "                df.loc[idx, 'meta_up_end']   = meta_hit_set[idx][0][1]\n",
    "\n",
    "            # How much does the up-stream Meta overlap the HGT block?\n",
    "            df['up_len_overlap'] = (\n",
    "                np.minimum(df['hgt_end'],  df['meta_up_end'])   # right bound\n",
    "                - np.maximum(df['hgt_start'], df['meta_up_start'])  # left bound\n",
    "            ).clip(lower=0)   # → 0 if there’s no overlap at all\n",
    "\n",
    "            # Keep rows only if the Meta block is *before* the HGT\n",
    "            # and they overlap by < 15 positions\n",
    "            df = df[\n",
    "                (df.meta_up_start < df.hgt_start) &\n",
    "                (df.up_len_overlap < 15)\n",
    "            ]\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # (3)  Check the *down-stream* Meta block (C-terminal side)\n",
    "        # -------------------------------------------------------------\n",
    "        if i < len(d) - 1 and df.shape[0]>0 and d[intervals[i+1]] == 'Meta':\n",
    "            ints = str(intervals[i+1]).replace(\" \", \"\")\n",
    "            meta_hit_set = get_arthropod_hit_set(f\"{ch};Meta_{ints}\")\n",
    "\n",
    "            for idx in set(meta_hit_set) & set(df.index):\n",
    "                df.loc[idx, 'meta_down_start'] = meta_hit_set[idx][0][0]\n",
    "                df.loc[idx, 'meta_down_end']   = meta_hit_set[idx][0][1]\n",
    "\n",
    "            # Overlap length between HGT and down-stream Meta block\n",
    "            df['down_len_overlap'] = (\n",
    "                np.minimum(df['hgt_end'],  df['meta_down_end'])\n",
    "                - np.maximum(df['hgt_start'], df['meta_down_start'])\n",
    "            ).clip(lower=0)\n",
    "\n",
    "            # Keep rows only if the Meta block is *after* the HGT\n",
    "            # and they overlap by < 15 positions\n",
    "            df = df[\n",
    "                (df.meta_down_start > df.hgt_start) &\n",
    "                (df.down_len_overlap < 15)\n",
    "            ]\n",
    "\n",
    "    return list(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f6ce80e-69df-45c2-b8b2-1820c95dd375",
   "metadata": {},
   "outputs": [],
   "source": [
    "##dictionary between primary chimera and its secondary chimeras\n",
    "secondary_chimera_adjacency_list={}\n",
    "for x in chimeras:\n",
    "    secondary_chimera_adjacency_list[x]=confirm_secondary_chimera_interval_order(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8df20f5c-3928-4018-bbdc-4adf5e2aec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save pickle dictionary representation of adj list output\n",
    "file_path = 'outputs/secondary_chimera_adjacency_list.pickle'\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(secondary_chimera_adjacency_list,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "052eaa9b-73c0-4078-9130-b6e0c3b8ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save pickle dictionary representation of adj list output\n",
    "file_path = 'outputs/secondary_chimera_adjacency_list.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    secondary_chimera_adjacency_list=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9175ed-fc67-41da-8b43-3857c064e063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##save .txt representation of adj list output\n",
    "f=open('outputs/secondary_chimera_adjacency_list.txt','w')\n",
    "for k,v in secondary_chimera_adjacency_list.items():\n",
    "    f.write(f\"{k}:{v}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab490d4-03de-4479-af84-4882e0a6dfb0",
   "metadata": {},
   "source": [
    "## Orthologous clustering of HGT-chimeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce070e90-1b62-4805-91ff-1a5822542646",
   "metadata": {},
   "outputs": [],
   "source": [
    "##adjacency list only including primary sequences\n",
    "secondary_chimera_adjacency_list_filtered={}\n",
    "for x in secondary_chimera_adjacency_list:\n",
    "    secondary_chimera_adjacency_list_filtered[x]=list(set(chimeras)&set(secondary_chimera_adjacency_list[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c98b24d7-4db3-4af6-a15d-e99653aefc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "##load fasta with chimeras from the previous iteration to prioritize in selection of representative sequences per cluster\n",
    "og = SeqIO.to_dict(SeqIO.parse('outputs/previous_iteration_chimeras.fa', 'fasta'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff966104-99f3-4d89-91a2-c660ca0d12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "##Build a graph from the adjacency list\n",
    "G = nx.DiGraph((u, v) for u, nbrs in secondary_chimera_adjacency_list_filtered.items() for v in nbrs)\n",
    "components = list(nx.weakly_connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec7a89d2-0c12-42de-b2a7-c73f005b79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_map={}\n",
    "i=0\n",
    "m=[]\n",
    "for c in components:\n",
    "    \n",
    "    ##prioritize previously sequenced chimeras as the cluster representatives\n",
    "    ## and those with hmmer hits\n",
    "    if len(c&set(og.keys()))>0:\n",
    "        \n",
    "        hmmer_hit=list(non_singleton_chimeras&c&set(og))\n",
    "        if len(hmmer_hit)>0:\n",
    "            k=hmmer_hit[0]\n",
    "            \n",
    "        else:\n",
    "            k=list(c&set(og.keys()))[0]\n",
    "        representative_map[k]=c\n",
    "        m.append(k)\n",
    "        i+=1\n",
    "    ##else slelect as the cluster representative the sequence with the maximum number of secondaries\n",
    "    else:\n",
    "        if len(c&non_singleton_chimeras)>0:\n",
    "            d={x:secondary_chimera_adjacency_list[x] for x in set(c)&set(non_singleton_chimeras)}\n",
    "        else:\n",
    "            d={x:secondary_chimera_adjacency_list[x] for x in set(c)}\n",
    "        max_len = max(map(len, d.values()))\n",
    "\n",
    "        max_sec= [k for k, v in d.items() if len(v) == max_len][0]\n",
    "        representative_map[max_sec]=c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9b0e363-f2d9-44ef-888c-0181c1163389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##print the number of clusters\n",
    "len(set(representative_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5496702d-00e1-4974-a981-0b21f522bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'outputs/clustering_representative_seqs.pickle'\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(representative_map,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54237547-eb02-4ff5-bb12-2e216ee38db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save .txt representation of adj list output\n",
    "f=open('outputs/clustering_representative_seqs.txt','w')\n",
    "for k,v in representative_map.items():\n",
    "    f.write(f\"{k}:{v}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4989d16a-920b-498f-ac93-7fab0955a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'outputs/clustering_representative_seqs.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    representative_map=pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf60ff-5d07-4e32-bf8b-38a393de8402",
   "metadata": {},
   "source": [
    "## Secondary chimera blast confirmation\n",
    "Uses DIAMOND blast to validate HGT or Metazoan annotations for each separated secondary chimera interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "20613c5d-2ae9-420b-a1a7-2c5750783272",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p outputs/phylogenetic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12dd3d4c-a858-4b79-9cd6-812c39da003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load a dictionary of primary:secondary chimera mappings for PCR-validated secondary chimeras from a previous pipeline iteration\n",
    "## this is to prioritize selection of pcr'd secondary chimeras (iff they appear as secondary chimeras in this screen)\n",
    "import pickle\n",
    "file_path = 'outputs/previous_iteration_secondary_chimeras.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    previous_iteration_secondary=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c6d462-49ca-4694-9fef-2b33e47e5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p outputs/secondary_chimera_fastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "707ec401-a6f0-4b12-9490-7bc9ac54f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "takes a chimera accession and writes a 'secondary_chimera' fasta for each interval\n",
    "with the hmmsearch-demarcated intervals of the secondary chimera \n",
    "\"\"\"\n",
    "def write_secondary_chimeras(c):\n",
    "    os.makedirs(f\"outputs/secondary_chimera_fastas/{c}\", exist_ok=True)\n",
    "    ints=[x for x in intervals if c in x]\n",
    "    for x in ints:\n",
    "        \n",
    "        os.makedirs(f\"outputs/secondary_chimera_fastas/{c}/{x}\", exist_ok=True)\n",
    "        \n",
    "        f=open(f\"outputs/secondary_chimera_fastas/{c}/{x}/secondary_chimera.fa\",'w')\n",
    "        \n",
    "        ##selection if using blast hits\n",
    "        if x in singleton_blast_hits:\n",
    "            arth_path = Path(f\"outputs/round2_diamond_v_arthropod_output_split/{x}.tsv\")\n",
    "            arth = pd.read_csv(arth_path, sep=\"\\t\")\n",
    "            arth=arth[arth.sseqid.isin(secondary_chimera_adjacency_list[c])]\n",
    "            arth=arth[arth.sseqid!=c]\n",
    "            arth=arth.loc[arth.groupby('sseqid')['bitscore'].idxmax()]\n",
    "            if arth.shape[0]>0:\n",
    "                for index, row in arth.iterrows():\n",
    "                    name=row.sseqid\n",
    "                    start=row.sstart\n",
    "                    stop=row.send\n",
    "                    seq=str(all_seqs[name].seq)[start-1:stop]\n",
    "                    name=name+\";\"+str((start,stop)).replace(\" \",\"\")\n",
    "                    f.write(f'>{name}\\n')\n",
    "                    f.write(f'{seq}\\n')\n",
    "        ##selection if using hmmer hits\n",
    "        else:\n",
    "            arth_path = Path(f\"outputs/hmmsearch_v_arthropod/{x}.tsv\")\n",
    "            arth = pd.read_csv(arth_path, sep=\"\\t\")\n",
    "            arth=arth[arth.target_name.isin(secondary_chimera_adjacency_list[c])]\n",
    "            arth=arth[arth.target_name!=c]\n",
    "            arth=arth.loc[arth.groupby('target_name')['domain_score'].idxmax()]\n",
    "            if arth.shape[0]>0:\n",
    "                for index, row in arth.iterrows():\n",
    "                    name=row.target_name\n",
    "                    start=row.envfrom\n",
    "                    stop=row.envto\n",
    "                    seq=str(all_seqs[name].seq)[start-1:stop]\n",
    "                    name=name+\";\"+str((start,stop)).replace(\" \",\"\")\n",
    "                    f.write(f'>{name}\\n')\n",
    "                    f.write(f'{seq}\\n')\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1a0c9ec-f07d-470e-9849-905152fa023c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _safe_write(c):\n",
    "    \"\"\"Wrapper so failures on a worker don’t crash the whole pool.\"\"\"\n",
    "    try:\n",
    "        write_secondary_chimeras(c)\n",
    "    except Exception:\n",
    "        # keep original behaviour: just show the item that failed\n",
    "        print(c)\n",
    "\n",
    "with mp.Pool(30) as pool:         # defaults to mp.cpu_count() workers\n",
    "    pool.map(_safe_write, list(representative_map.keys()))\n",
    "        # pool.map automatically waits for all tasks to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef766f4b-1e37-4578-ad1d-f5e492a8299c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root      = Path(\"outputs/secondary_chimera_fastas\")          # adjust if needed\n",
    "outfile   = Path(\"outputs/all_secondary_chimeras.fa\")      # where to write\n",
    "\n",
    "with outfile.open(\"w\") as out:\n",
    "    # pattern: root / * / * / secondary_chimera.fa\n",
    "    for fasta in root.glob(\"*/*/secondary_chimera.fa\"):\n",
    "        tag = fasta.parent.name                   # immediate subdirectory\n",
    "        with fasta.open() as fh:\n",
    "            for line in fh:\n",
    "                if line.startswith(\">\"):\n",
    "                    # strip the leading '>' and trailing newline, then rewrite\n",
    "                    out.write(f\">{tag};;{line[1:].rstrip()}\\n\")\n",
    "                else:\n",
    "                    out.write(line)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bb10a1e-555b-462c-94ab-8dc3d7b52049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 17319553\n"
     ]
    }
   ],
   "source": [
    "!sbatch \"scripts/diamond_secondary.sh\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "261dd9ae-fc0c-4ac5-941c-96b1fb72a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh scripts/split_blast_table.sh 'all_secondary_chimeras_out' 'outputs/secondary_chimera_interval_blast_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49ad35e-4817-4a9c-a386-51082c9b819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_dict=SeqIO.to_dict(SeqIO.parse('outputs/all_secondary_chimeras.fa', 'fasta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd3d4e4-cc26-4215-8b79-34626a19544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes the name of an interval blast dataframe (string) stored in round2_diamond_output_split\n",
    "#returns \"Meta\", \"HGT\" or none\n",
    "def check_annot(n):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for every **secondary chimera** blast output,\n",
    "    takes the name of an interval blast dataframe (string) stored in secondary_chimera_interval_blast\n",
    "    returns \"Meta\", \"HGT\" or none\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        df=pd.read_csv(f\"/n/netscratch/extavour_lab/Everyone/Rishabh/secondary_chimera_interval_blast_results/{n}.tsv\",sep=\"\\t\", names=\"qseqid sseqid stitle staxids sscinames sphylums skingdoms pident length mismatch gapopen qstart qend sstart send evalue bitscore\".split(\" \"))\n",
    "\n",
    "        leng=len(record_dict[n].seq)\n",
    "        df[\"cov\"]=(np.array(df.qend)-np.array(df.qstart)+1)/leng\n",
    "        #filter by >30% coverage of the query\n",
    "        dfo=df[df[\"cov\"]>.30]\n",
    "        dfo=dfo[~dfo.sphylums.astype(str).str.contains(\"Arthropoda\")]\n",
    "        dfo=dfo[~dfo.sphylums.astype(str).str.contains(\"Rotifera\")]\n",
    "        dfo=dfo[dfo.staxids.astype(str)!=\"nan\"]\n",
    "        ##exclude synthetic sequences\n",
    "        dfm=dfo[dfo.staxids!=32630]\n",
    "\n",
    "        dfmeta=dfm[dfm.skingdoms.astype(str).str.contains(\"Metazoa\")]\n",
    "        dfhgt=dfm[~dfm.skingdoms.astype(str).str.contains(\"Metazoa\")]\n",
    "        dfhgt[\"AI\"]=np.log10(dfmeta.evalue.min()+1e-200)-np.log10(dfhgt.evalue+1e-200)\n",
    "        dfmeta[\"MI\"]=np.log10(dfhgt.evalue.min()+1e-200)-np.log10(dfmeta.evalue+1e-200)\n",
    "\n",
    "        ##get the top 300 hits by lowest evalue\n",
    "        dfmi=dfm.iloc[0:300,:]\n",
    "        dfmetai=dfmi[dfmi.skingdoms.astype(str).str.contains(\"Metazoa\")]\n",
    "        dfhgti=dfmi[~dfmi.skingdoms.astype(str).str.contains(\"Metazoa\")]\n",
    "\n",
    "\n",
    "        hgt_condition= (dfhgt.evalue.min()<1e-4 or dfhgt.bitscore.max()>50) and len(set(dfhgt.staxids))>10 and (len(set(dfhgt[dfhgt.AI>5].staxids))>10 or len(set(dfhgti.staxids))/len(set(dfmi.staxids))>=.95)\n",
    "        meta_condition= dfmeta.evalue.min()<.1  and (len(set(dfmeta[dfmeta.MI>1].staxids))>5 or (len(set(dfmetai.staxids))/len(set(dfmi.staxids))>=.50))\n",
    "        if dfm.shape[0]>0:\n",
    "            # print(dfhgt.evalue.min(), dfmetai.shape[0])\n",
    "            if meta_condition:\n",
    "                return \"Meta\"\n",
    "            elif hgt_condition:\n",
    "                return \"HGT\"\n",
    "        del df,dfo, dfm,dfmeta, dfhgt, dfmi, dfmetai, dfhgti\n",
    "    except:\n",
    "        print(n,'error')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ee0c7e-98dd-4f99-b347-d72b502cac95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCF_025399875.1;XP_065172373.1;Meta_(688,743);;GCF_025399875.1;XP_065172344.1;(700,765) error\n",
      "GCF_025399875.1;XP_065172373.1;Meta_(688,743);;GCF_025399875.1;XP_065172345.1;(700,765) error\n",
      "GCF_025399875.1;XP_065172373.1;Meta_(688,743);;GCF_025399875.1;XP_065172507.1;(626,690) error\n",
      "GCF_025399875.1;XP_065172373.1;Meta_(688,743);;GCF_025399875.1;XP_065172508.1;(626,690) error\n"
     ]
    }
   ],
   "source": [
    "td=list(record_dict.keys())\n",
    "with mp.Pool(55) as pool:         # defaults to mp.cpu_count() workers\n",
    "    result = pool.map(check_annot, td)\n",
    "secondary_interval_annots={x:y for x,y in zip(td,result)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f063918f-7337-40d5-a50a-0db1e8d97ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interv_td=set([x.split(\";;\")[0] for x in td])\n",
    "##dictionary storing the percentage of secondary chimeras with mismatching annotations for each chimera interval\n",
    "p_mismatch={}\n",
    "opposite_annot={}\n",
    "for interv in interv_td:\n",
    "\n",
    "    annot=interv.split(\";\")[-1].split(\"_\")[0]\n",
    "    all_secondaries={secondary_interval_annots[x] for x in secondary_interval_annots if interv in x}\n",
    "    opposite={secondary_interval_annots[x] for x in secondary_interval_annots if interv in x and secondary_interval_annots[x]!=annot and secondary_interval_annots[x]!=None}\n",
    "    p_mismatch[interv]=len(opposite)/len(all_secondaries)\n",
    "    opposite_annot[interv]=opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c165f0-506f-4f83-af32-883b31cd22b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##chimeras excluded because annotations of secondary chimeras switched from hgt to meta or vice-a-versa\n",
    "to_exclude=set([\";\".join(x.split(\";\")[0:2]) for x in p_mismatch if p_mismatch[x]>0 and \";\".join(x.split(\";\")[0:2]) in representative_map])\n",
    "len(to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98db3bc3-b5ce-4ff1-b479-516a13948f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "##final set of clustered-hgt chimeras\n",
    "hmmer_chimeras_to_include=set(representative_map)-set(to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a47b5310-bd18-473c-ac99-0d33e2a37eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hmmer_chimeras_to_include)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f4d50-16f7-444c-9a4f-a90da15bc291",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data table output\n",
    "Makes a table of all final, seconday-chimera filtered outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f07bee10-dec8-481d-92c1-74f784c8456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load a dataframe of genome taxids from genome accessions\n",
    "df1=pd.read_csv('Data/genbank_genomes_4_22_2025.tsv',sep='\\t')\n",
    "df2=pd.read_csv('Data/refseq_genomes_scaffold_plus_4_19_2025.tsv',sep='\\t')\n",
    "dftax=pd.concat([df1,df2]).set_index('Assembly Accession')\n",
    "dftax.loc['GCF_006496715.1',['Organism Name','Organism Taxonomic ID']]=['Aedes albopictus',7160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acccf87e-45e5-4a71-a5e9-7b524213be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import NCBITaxa\n",
    "\n",
    "\n",
    "ncbi = NCBITaxa()          \n",
    "\n",
    "def lowest_common_rank(taxids, ncbi_obj=ncbi):\n",
    "    \"\"\"\n",
    "    Return the (taxid, scientific name, rank) of the deepest/commonest rank\n",
    "    that all input NCBI taxids share.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    taxids : Iterable[int]\n",
    "        A sequence or set of NCBI taxonomy IDs (e.g. {9606, 10090, 9598}).\n",
    "    ncbi_obj : ete3.NCBITaxa, optional\n",
    "        Pre-instantiated NCBITaxa object (default: the module-level `ncbi`).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple (int, str, str)\n",
    "        taxid, scientific name, and rank of the lowest common ancestor (LCA).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `taxids` is empty or no common ancestor is found (shouldn’t happen\n",
    "        unless an ID is not in the database).\n",
    "    \"\"\"\n",
    "    taxids = list({int(t) for t in taxids})   # unique & cast to int\n",
    "    if not taxids:\n",
    "        raise ValueError(\"`taxids` must contain at least one ID\")\n",
    "\n",
    "    # Full lineage (root → leaf) for each taxid\n",
    "    lineages = []\n",
    "    for x in taxids:\n",
    "        try:\n",
    "            lineages.append(ncbi_obj.get_lineage(x))\n",
    "        except:\n",
    "            print(x)\n",
    "\n",
    "    # All ancestors common to every lineage\n",
    "    common = set(lineages[0]).intersection(*lineages[1:])\n",
    "    if not common:\n",
    "        raise ValueError(\"No common ancestor found – check the taxids.\")\n",
    "\n",
    "    # Depth of each ancestor in the first lineage: 0=root, larger=deeper\n",
    "    depth = {tax: idx for idx, tax in enumerate(lineages[0])}\n",
    "\n",
    "    # Pick the common taxon that’s deepest in the tree\n",
    "    lca_taxid = max(common, key=lambda t: depth[t])\n",
    "\n",
    "    # Translate to name and rank\n",
    "    name = ncbi_obj.get_taxid_translator([lca_taxid])[lca_taxid]\n",
    "    rank = ncbi_obj.get_rank([lca_taxid])[lca_taxid]\n",
    "\n",
    "    return lca_taxid, name, rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c985317-02cf-495d-aae7-3f8b24cc7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed0cb952-6331-44ba-8bc1-1d79e20a638d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3402493\n",
      "3402493\n"
     ]
    }
   ],
   "source": [
    "for x in hmmer_chimeras_to_include:\n",
    "    ## fill in taxonomic information   \n",
    "    species=[dftax.loc[xi.split(\";\")[0],'Organism Name'] for xi in secondary_chimera_adjacency_list[x] if xi.split(\";\")[0] in dftax.index ]\n",
    "    ## fill in taxids of secondary chimeras\n",
    "    taxids=list(set([int(dftax.loc[xi.split(\";\")[0],'Organism Taxonomic ID']) for xi in secondary_chimera_adjacency_list[x] if xi.split(\";\")[0] in dftax.index ]))\n",
    "    rank=''\n",
    "    try:\n",
    "        ## fill in the lowest common taxonomic rank of the secondary chimeras\n",
    "        rank=str(lowest_common_rank(taxids))\n",
    "\n",
    "    except:\n",
    "        print(x)\n",
    "    df.loc[x,['n_species','span','secondary_chimera_species','secondary_chimera_sequences']]=len(set(species)),rank,str(set(species)),str(set(secondary_chimera_adjacency_list[x]))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bdf7ea1-25f1-47b6-a2a8-82fb907efc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('n_species',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0893a4db-0c51-46dc-baad-428e0147c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    df.loc[index,'HGT_intervals']=str([x for x in chimeras[index] if chimeras[index][x]=='HGT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deb96a68-a277-4dc4-96d4-fdbb04ea06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    df.loc[index,'Meta_intervals']=str([x for x in chimeras[index] if chimeras[index][x]=='Meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3573c5c6-b05d-4b26-b4df-849a9a1b9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdhit=pd.read_csv(\"outputs/round2_chimeras_cdhit.txt\",sep=\"\\t\")\n",
    "cdhit['Query']=[x.split(\">\")[1] for x in cdhit['Query']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a69a057-6c6b-4729-87b5-aa06bf7f99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    ci=cdhit[cdhit.Query==index.split(';')[1]]\n",
    "    ci=ci.sort_values('From')\n",
    "    s=str([(x,y,z) for x,y,z in zip(ci['Short name'],ci['From'],ci['To'])])\n",
    "    df.loc[index,'cdhit']=s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c83dfded-9f9a-4c39-b425-c92e99319132",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    df.loc[index,'og']=index in og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0896a779-2a78-4d5d-80f2-3a9fe586e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('outputs/clustered_ankyrin_transposon_secondary_filtered_chimeras.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fde11096-f4cd-4cba-b657-f71e96c74057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = 'outputs/transposon_ankyrin_filtered_round2_chimera_intervals.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    chimeras=pickle.load(file)\n",
    "chimeras_filtered={x:chimeras[x] for x in chimeras if x in df.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85d4c9a8-0255-427e-a277-5edcd7a1ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save dictionary representation of filtered chimeras output\n",
    "file_path = 'outputs/clustered_ankyrin_transposon_secondary_filtered_chimeras.pickle'\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(chimeras_filtered,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ae770ce-986b-46cb-8a37-be39a540f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save .txt representation of filtered chimeras output\n",
    "f=open('outputs/clustered_ankyrin_transposon_secondary_filtered_chimeras.txt','w')\n",
    "for k,v in chimeras_filtered.items():\n",
    "    f.write(f\"{k}:{v}\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f013542-e1b4-4c02-8d58-895b5bd204a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rishabh]",
   "language": "python",
   "name": "conda-env-.conda-rishabh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
